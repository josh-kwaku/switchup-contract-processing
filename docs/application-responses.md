# Application Responses

## What improvements you've recently made to your AI-enabled workflow: what worked surprisingly well, what's still holding you back?

My AI-enabled workflow has changed substantially over the past year. The biggest shift was moving from using AI as a code autocomplete to treating it as a collaborator that needs structure, the same way you'd onboard a productive but inexperienced engineer.

The workflow I've converged on starts before any code is written. I use Claude Code's `AskUserQuestion` tool to have it interview me about the problem: what are we solving, who's it for, what are the constraints. This produces a PRD. Then we move to technical design: architecture, schema, state machine, error handling strategy, all documented in decision records before implementation begins. Only then do I break the work into sprints, where each sprint produces demoable software and each task is an atomic, committable unit with clear validation criteria.

What worked surprisingly well: a project memory system. I maintain a `CLAUDE.md` file that acts as a persistent system prompt, encoding session protocols, coding standards, and mandatory review gates. A `SESSION.md` serves as a handoff document between sessions. I enforce explicit guideline documents that the AI must follow: `CODE_GUIDELINES.md`, `API_GUIDELINES.md`, `ERROR_HANDLING.md`, and `OBSERVABILITY_GUIDELINES.md`. I also built a custom code review subagent that automatically checks every task against these guidelines before I review it myself. The critical pattern is stopping after every task for human review. AI compounds errors when you let it run unsupervised across multiple tasks, so I enforce a hard gate: implement one task, review, approve, then proceed.

What's still holding me back: context degradation across sessions. The session handoff files are a workaround, not a solution. The AI doesn't truly retain understanding from prior sessions. And architectural judgment remains weak. AI is excellent at implementing a pattern you've chosen but unreliable at choosing between patterns. That's why I force design discussions and ADRs before any code. It keeps the human in the loop on the decisions that matter most, and frees the AI to execute on the decisions that have already been made.

---

## A system you've built that's similar to what we're facing: complex workflows, state machines, highly configurable products. What was the problem, and what made it hard?

At Alteos (an embedded insurance company), I led the design and delivery of a partner management platform and automated commission processing system for 3000+ insurance distribution partners. The core problem was that partner data was scattered across spreadsheets, CSVs, and multiple systems, and commission processing took the finance team 4 days of manual work every month.

The hardest part was the abstraction layer. Policy sales used 8+ different fields to identify who sold a policy, each introduced by different partner integrations over time. New fields could appear silently during product configuration, causing the commission system to misattribute sales without anyone knowing. I designed a standardized identification system (`entityCode`/`contractCode`) to collapse this complexity, but couldn't migrate 3000+ partners overnight. So I built a bridge: a nightly attribution job that analyses each day's policy sales, extracts seller identification from whichever legacy field was used, matches it against the partner platform, and correctly attributes the sale. When an entity isn't found, it alerts operations via Slack with affected policy IDs, a human-in-the-loop pattern where automation handles the common case and people handle the exceptions.

The commission engine itself needed to be highly configurable, supporting tiered commission structures, deferrals, multi-party splits across beneficiaries and branch hierarchies, while being robust enough to process commissions correctly for every partner every month. I used Redis-based distributed locking with idempotency keys to ensure exactly-once processing across concurrent calculations. The result: 4 days of manual work reduced to under 1 hour, 2 FTE of reconciliation eliminated.

What made this hard wasn't primarily technical. It was the "two worlds" problem. Product Configuration and Partner Management had evolved independently, each with their own data models and assumptions. You couldn't redesign either in isolation. The solution required building bridges between the systems while simultaneously migrating toward standardization, and coordinating across operations, finance, product management, and engineering to make it work. I wrote a comprehensive internal document to align all stakeholders on the current state, the migration path, and each team's role, because the technical architecture was only as good as the cross-functional buy-in behind it.
